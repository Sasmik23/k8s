
==> Audit <==
|---------|-------------------------|----------|--------|---------|---------------------|---------------------|
| Command |          Args           | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|-------------------------|----------|--------|---------|---------------------|---------------------|
| start   |                         | minikube | mikhil | v1.33.1 | 10 Jun 24 17:42 +08 | 10 Jun 24 17:44 +08 |
| service | flask-app-service --url | minikube | mikhil | v1.33.1 | 10 Jun 24 17:47 +08 |                     |
| service | flask-app-service --url | minikube | mikhil | v1.33.1 | 10 Jun 24 17:48 +08 |                     |
|---------|-------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/10 17:42:34
Running on machine: Mikhils-MacBook-Pro
Binary: Built with gc go1.22.3 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0610 17:42:34.460747   71528 out.go:291] Setting OutFile to fd 1 ...
I0610 17:42:34.460948   71528 out.go:343] isatty.IsTerminal(1) = true
I0610 17:42:34.460950   71528 out.go:304] Setting ErrFile to fd 2...
I0610 17:42:34.460953   71528 out.go:343] isatty.IsTerminal(2) = true
I0610 17:42:34.461055   71528 root.go:338] Updating PATH: /Users/mikhil/.minikube/bin
W0610 17:42:34.461153   71528 root.go:314] Error reading config file at /Users/mikhil/.minikube/config/config.json: open /Users/mikhil/.minikube/config/config.json: no such file or directory
I0610 17:42:34.463082   71528 out.go:298] Setting JSON to false
I0610 17:42:34.486222   71528 start.go:129] hostinfo: {"hostname":"Mikhils-MacBook-Pro.local","uptime":226785,"bootTime":1717785769,"procs":590,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.5","kernelVersion":"23.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"d539ef7b-4177-5c50-b951-828a430ee5a2"}
W0610 17:42:34.486318   71528 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0610 17:42:34.492027   71528 out.go:177] ðŸ˜„  minikube v1.33.1 on Darwin 14.5 (arm64)
I0610 17:42:34.508183   71528 notify.go:220] Checking for updates...
W0610 17:42:34.508200   71528 preload.go:294] Failed to list preload files: open /Users/mikhil/.minikube/cache/preloaded-tarball: no such file or directory
I0610 17:42:34.508355   71528 driver.go:392] Setting default libvirt URI to qemu:///system
I0610 17:42:34.508413   71528 global.go:112] Querying for installed drivers using PATH=/Users/mikhil/.minikube/bin:/opt/homebrew/bin:/opt/homebrew/bin:/Library/Frameworks/Python.framework/Versions/3.9/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/Wireshark.app/Contents/MacOS:/usr/local/go/bin:/opt/homebrew/bin:/Users/mikhil/anaconda3/bin:/Users/mikhil/anaconda3/condabin:/Library/Frameworks/Python.framework/Versions/3.9/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin
I0610 17:42:34.508540   71528 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0610 17:42:34.576234   71528 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0610 17:42:34.576362   71528 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0610 17:42:34.746454   71528 info.go:266] docker info: {ID:f2fea559-856a-475a-b2c0-3c154ffa056e Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:false NGoroutines:84 SystemTime:2024-06-10 09:42:34.730178178 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8221949952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/mikhil/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/mikhil/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/mikhil/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/mikhil/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/mikhil/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/mikhil/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/mikhil/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/mikhil/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/mikhil/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/mikhil/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0610 17:42:34.746542   71528 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0610 17:42:34.746732   71528 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0610 17:42:34.746750   71528 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0610 17:42:34.746811   71528 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0610 17:42:34.746857   71528 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0610 17:42:34.746932   71528 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0610 17:42:34.747017   71528 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0610 17:42:34.747030   71528 driver.go:314] not recommending "ssh" due to default: false
I0610 17:42:34.747038   71528 driver.go:349] Picked: docker
I0610 17:42:34.747041   71528 driver.go:350] Alternatives: [ssh]
I0610 17:42:34.747043   71528 driver.go:351] Rejects: [vmware podman hyperkit parallels qemu2 virtualbox]
I0610 17:42:34.757769   71528 out.go:177] âœ¨  Automatically selected the docker driver
I0610 17:42:34.761803   71528 start.go:297] selected driver: docker
I0610 17:42:34.761806   71528 start.go:901] validating driver "docker" against <nil>
I0610 17:42:34.761824   71528 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0610 17:42:34.761898   71528 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0610 17:42:34.849529   71528 info.go:266] docker info: {ID:f2fea559-856a-475a-b2c0-3c154ffa056e Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:false NGoroutines:84 SystemTime:2024-06-10 09:42:34.833907136 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8221949952 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/mikhil/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/mikhil/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/Users/mikhil/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/Users/mikhil/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/Users/mikhil/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/mikhil/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/mikhil/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/mikhil/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/mikhil/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/mikhil/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0610 17:42:34.849663   71528 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0610 17:42:34.849792   71528 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16384MB, container=7841MB
I0610 17:42:34.849902   71528 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0610 17:42:34.853680   71528 out.go:177] ðŸ“Œ  Using Docker Desktop driver with root privileges
I0610 17:42:34.857814   71528 cni.go:84] Creating CNI manager for ""
I0610 17:42:34.857831   71528 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0610 17:42:34.857844   71528 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0610 17:42:34.857886   71528 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0610 17:42:34.865798   71528 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0610 17:42:34.869790   71528 cache.go:121] Beginning downloading kic base image for docker with docker
I0610 17:42:34.874815   71528 out.go:177] ðŸšœ  Pulling base image v0.0.44 ...
I0610 17:42:34.883805   71528 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0610 17:42:34.883805   71528 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0610 17:42:34.930487   71528 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0610 17:42:34.930760   71528 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory
I0610 17:42:34.930879   71528 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0610 17:42:35.727557   71528 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0610 17:42:35.727593   71528 cache.go:56] Caching tarball of preloaded images
I0610 17:42:35.728241   71528 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0610 17:42:35.733992   71528 out.go:177] ðŸ’¾  Downloading Kubernetes v1.30.0 preload ...
I0610 17:42:35.744953   71528 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0610 17:42:36.893404   71528 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4?checksum=md5:677034533668c42fec962cc52f9b3c42 -> /Users/mikhil/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0610 17:43:12.156079   71528 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0610 17:43:12.156254   71528 preload.go:255] verifying checksum of /Users/mikhil/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0610 17:43:12.787099   71528 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0610 17:43:12.787399   71528 profile.go:143] Saving config to /Users/mikhil/.minikube/profiles/minikube/config.json ...
I0610 17:43:12.787416   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/config.json: {Name:mk12a1d572fe826623ff73057015d31b33114c27 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:43:34.276516   71528 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e as a tarball
I0610 17:43:34.276545   71528 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from local cache
I0610 17:43:46.476224   71528 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from cached tarball
I0610 17:43:46.478052   71528 cache.go:194] Successfully downloaded all kic artifacts
I0610 17:43:46.478978   71528 start.go:360] acquireMachinesLock for minikube: {Name:mk4c210d572998020a3eb9a47da42e3171a76337 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0610 17:43:46.480166   71528 start.go:364] duration metric: took 821.75Âµs to acquireMachinesLock for "minikube"
I0610 17:43:46.480360   71528 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0610 17:43:46.481229   71528 start.go:125] createHost starting for "" (driver="docker")
I0610 17:43:46.500425   71528 out.go:204] ðŸ”¥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0610 17:43:46.501453   71528 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0610 17:43:46.501495   71528 client.go:168] LocalClient.Create starting
I0610 17:43:46.504365   71528 main.go:141] libmachine: Creating CA: /Users/mikhil/.minikube/certs/ca.pem
I0610 17:43:46.573818   71528 main.go:141] libmachine: Creating client certificate: /Users/mikhil/.minikube/certs/cert.pem
I0610 17:43:46.625308   71528 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0610 17:43:46.678116   71528 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0610 17:43:46.678230   71528 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0610 17:43:46.678245   71528 cli_runner.go:164] Run: docker network inspect minikube
W0610 17:43:46.722876   71528 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0610 17:43:46.722904   71528 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0610 17:43:46.722916   71528 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0610 17:43:46.723048   71528 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0610 17:43:46.822043   71528 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14001669ec0}
I0610 17:43:46.822088   71528 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0610 17:43:46.822162   71528 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0610 17:43:46.970650   71528 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0610 17:43:46.970693   71528 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0610 17:43:46.970809   71528 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0610 17:43:47.013209   71528 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0610 17:43:47.060278   71528 oci.go:103] Successfully created a docker volume minikube
I0610 17:43:47.060388   71528 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib
I0610 17:43:49.026376   71528 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib: (1.9659375s)
I0610 17:43:49.026415   71528 oci.go:107] Successfully prepared a docker volume minikube
I0610 17:43:49.026446   71528 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0610 17:43:49.026486   71528 kic.go:194] Starting extracting preloaded images to volume ...
I0610 17:43:49.026649   71528 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/mikhil/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir
I0610 17:43:53.014326   71528 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/mikhil/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir: (3.987522458s)
I0610 17:43:53.014465   71528 kic.go:203] duration metric: took 3.988030209s to extract preloaded images to volume ...
I0610 17:43:53.015237   71528 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0610 17:43:53.376179   71528 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e
I0610 17:43:54.126824   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0610 17:43:54.271277   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:43:54.337702   71528 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0610 17:43:54.620471   71528 oci.go:144] the created container "minikube" has a running status.
I0610 17:43:54.620495   71528 kic.go:225] Creating ssh key for kic: /Users/mikhil/.minikube/machines/minikube/id_rsa...
I0610 17:43:54.689625   71528 kic_runner.go:191] docker (temp): /Users/mikhil/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0610 17:43:54.761210   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:43:54.841586   71528 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0610 17:43:54.841610   71528 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0610 17:43:55.029735   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:43:55.087421   71528 machine.go:94] provisionDockerMachine start ...
I0610 17:43:55.087620   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:55.159233   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:55.166099   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:55.166108   71528 main.go:141] libmachine: About to run SSH command:
hostname
I0610 17:43:55.377742   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0610 17:43:55.377774   71528 ubuntu.go:169] provisioning hostname "minikube"
I0610 17:43:55.377880   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:55.435566   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:55.435804   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:55.435814   71528 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0610 17:43:55.576683   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0610 17:43:55.576787   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:55.628246   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:55.628464   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:55.628470   71528 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0610 17:43:55.756179   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0610 17:43:55.756218   71528 ubuntu.go:175] set auth options {CertDir:/Users/mikhil/.minikube CaCertPath:/Users/mikhil/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/mikhil/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/mikhil/.minikube/machines/server.pem ServerKeyPath:/Users/mikhil/.minikube/machines/server-key.pem ClientKeyPath:/Users/mikhil/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/mikhil/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/mikhil/.minikube}
I0610 17:43:55.756241   71528 ubuntu.go:177] setting up certificates
I0610 17:43:55.756251   71528 provision.go:84] configureAuth start
I0610 17:43:55.756344   71528 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0610 17:43:55.811981   71528 provision.go:143] copyHostCerts
I0610 17:43:55.812282   71528 exec_runner.go:151] cp: /Users/mikhil/.minikube/certs/ca.pem --> /Users/mikhil/.minikube/ca.pem (1078 bytes)
I0610 17:43:55.813112   71528 exec_runner.go:151] cp: /Users/mikhil/.minikube/certs/cert.pem --> /Users/mikhil/.minikube/cert.pem (1123 bytes)
I0610 17:43:55.813605   71528 exec_runner.go:151] cp: /Users/mikhil/.minikube/certs/key.pem --> /Users/mikhil/.minikube/key.pem (1679 bytes)
I0610 17:43:55.813948   71528 provision.go:117] generating server cert: /Users/mikhil/.minikube/machines/server.pem ca-key=/Users/mikhil/.minikube/certs/ca.pem private-key=/Users/mikhil/.minikube/certs/ca-key.pem org=mikhil.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0610 17:43:55.889192   71528 provision.go:177] copyRemoteCerts
I0610 17:43:55.889451   71528 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0610 17:43:55.889488   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:55.941661   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:43:56.033325   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0610 17:43:56.053580   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0610 17:43:56.070702   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0610 17:43:56.085364   71528 provision.go:87] duration metric: took 329.10775ms to configureAuth
I0610 17:43:56.085376   71528 ubuntu.go:193] setting minikube options for container-runtime
I0610 17:43:56.085866   71528 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0610 17:43:56.085927   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:56.142938   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:56.143164   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:56.143168   71528 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0610 17:43:56.273148   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0610 17:43:56.273157   71528 ubuntu.go:71] root file system type: overlay
I0610 17:43:56.273233   71528 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0610 17:43:56.273334   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:56.346366   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:56.346552   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:56.346587   71528 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0610 17:43:56.497031   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0610 17:43:56.497236   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:56.559756   71528 main.go:141] libmachine: Using SSH client type: native
I0610 17:43:56.560319   71528 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1013ab180] 0x1013ad9e0 <nil>  [] 0s} 127.0.0.1 56204 <nil> <nil>}
I0610 17:43:56.560327   71528 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0610 17:43:58.106314   71528 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-30 11:46:21.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-06-10 09:43:56.493632007 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0610 17:43:58.106375   71528 machine.go:97] duration metric: took 3.018970458s to provisionDockerMachine
I0610 17:43:58.106460   71528 client.go:171] duration metric: took 11.605119916s to LocalClient.Create
I0610 17:43:58.106522   71528 start.go:167] duration metric: took 11.605241s to libmachine.API.Create "minikube"
I0610 17:43:58.106529   71528 start.go:293] postStartSetup for "minikube" (driver="docker")
I0610 17:43:58.106551   71528 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0610 17:43:58.106783   71528 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0610 17:43:58.107047   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:58.169293   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:43:58.274040   71528 ssh_runner.go:195] Run: cat /etc/os-release
I0610 17:43:58.280181   71528 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0610 17:43:58.280217   71528 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0610 17:43:58.280226   71528 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0610 17:43:58.280233   71528 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0610 17:43:58.280243   71528 filesync.go:126] Scanning /Users/mikhil/.minikube/addons for local assets ...
I0610 17:43:58.280462   71528 filesync.go:126] Scanning /Users/mikhil/.minikube/files for local assets ...
I0610 17:43:58.280538   71528 start.go:296] duration metric: took 173.989542ms for postStartSetup
I0610 17:43:58.282596   71528 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0610 17:43:58.366917   71528 profile.go:143] Saving config to /Users/mikhil/.minikube/profiles/minikube/config.json ...
I0610 17:43:58.367458   71528 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0610 17:43:58.367522   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:58.412757   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:43:58.502799   71528 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0610 17:43:58.506883   71528 start.go:128] duration metric: took 12.025806292s to createHost
I0610 17:43:58.506892   71528 start.go:83] releasing machines lock for "minikube", held for 12.026778375s
I0610 17:43:58.506968   71528 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0610 17:43:58.551830   71528 ssh_runner.go:195] Run: cat /version.json
I0610 17:43:58.551910   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:58.554736   71528 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0610 17:43:58.554871   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:43:58.606184   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:43:58.612082   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:43:58.697802   71528 ssh_runner.go:195] Run: systemctl --version
I0610 17:43:58.855722   71528 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0610 17:43:58.859574   71528 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0610 17:43:58.875248   71528 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0610 17:43:58.875340   71528 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0610 17:43:58.894246   71528 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0610 17:43:58.894258   71528 start.go:494] detecting cgroup driver to use...
I0610 17:43:58.894274   71528 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0610 17:43:58.894740   71528 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0610 17:43:58.906332   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0610 17:43:58.911615   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0610 17:43:58.917550   71528 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0610 17:43:58.917695   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0610 17:43:58.930150   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0610 17:43:58.942138   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0610 17:43:58.947836   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0610 17:43:58.953175   71528 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0610 17:43:58.959793   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0610 17:43:58.965779   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0610 17:43:58.971324   71528 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0610 17:43:58.976949   71528 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0610 17:43:58.981845   71528 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0610 17:43:58.986772   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:43:59.046295   71528 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0610 17:43:59.168702   71528 start.go:494] detecting cgroup driver to use...
I0610 17:43:59.168730   71528 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0610 17:43:59.168985   71528 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0610 17:43:59.177797   71528 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0610 17:43:59.177953   71528 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0610 17:43:59.190713   71528 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0610 17:43:59.203360   71528 ssh_runner.go:195] Run: which cri-dockerd
I0610 17:43:59.215129   71528 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0610 17:43:59.221984   71528 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0610 17:43:59.237574   71528 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0610 17:43:59.286816   71528 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0610 17:43:59.381508   71528 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0610 17:43:59.389478   71528 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0610 17:43:59.408961   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:43:59.472625   71528 ssh_runner.go:195] Run: sudo systemctl restart docker
I0610 17:43:59.659034   71528 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0610 17:43:59.666140   71528 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0610 17:43:59.672451   71528 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0610 17:43:59.711507   71528 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0610 17:43:59.743059   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:43:59.775771   71528 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0610 17:43:59.799435   71528 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0610 17:43:59.810838   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:43:59.847915   71528 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0610 17:43:59.950665   71528 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0610 17:43:59.951429   71528 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0610 17:43:59.960356   71528 start.go:562] Will wait 60s for crictl version
I0610 17:43:59.960540   71528 ssh_runner.go:195] Run: which crictl
I0610 17:43:59.966386   71528 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0610 17:44:00.017669   71528 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0610 17:44:00.017812   71528 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0610 17:44:00.086827   71528 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0610 17:44:00.113947   71528 out.go:204] ðŸ³  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0610 17:44:00.114249   71528 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0610 17:44:00.283897   71528 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0610 17:44:00.284069   71528 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0610 17:44:00.287896   71528 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0610 17:44:00.295435   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0610 17:44:00.343598   71528 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0610 17:44:00.343706   71528 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0610 17:44:00.343790   71528 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0610 17:44:00.355620   71528 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0610 17:44:00.355632   71528 docker.go:615] Images already preloaded, skipping extraction
I0610 17:44:00.355716   71528 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0610 17:44:00.368767   71528 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0610 17:44:00.368775   71528 cache_images.go:84] Images are preloaded, skipping loading
I0610 17:44:00.368779   71528 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0610 17:44:00.368927   71528 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0610 17:44:00.368996   71528 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0610 17:44:00.419422   71528 cni.go:84] Creating CNI manager for ""
I0610 17:44:00.419436   71528 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0610 17:44:00.419443   71528 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0610 17:44:00.419461   71528 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0610 17:44:00.419583   71528 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0610 17:44:00.419678   71528 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0610 17:44:00.425106   71528 binaries.go:44] Found k8s binaries, skipping transfer
I0610 17:44:00.425213   71528 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0610 17:44:00.430305   71528 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0610 17:44:00.439335   71528 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0610 17:44:00.454144   71528 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0610 17:44:00.464898   71528 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0610 17:44:00.467556   71528 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0610 17:44:00.474644   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:44:00.511681   71528 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0610 17:44:00.535511   71528 certs.go:68] Setting up /Users/mikhil/.minikube/profiles/minikube for IP: 192.168.49.2
I0610 17:44:00.535556   71528 certs.go:194] generating shared ca certs ...
I0610 17:44:00.535572   71528 certs.go:226] acquiring lock for ca certs: {Name:mk2532e77d301b4d28d5f8ba691f33f33c40125d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.537175   71528 certs.go:240] generating "minikubeCA" ca cert: /Users/mikhil/.minikube/ca.key
I0610 17:44:00.625786   71528 crypto.go:156] Writing cert to /Users/mikhil/.minikube/ca.crt ...
I0610 17:44:00.625798   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/ca.crt: {Name:mk9ece153e429d886417ff38b72c707d84359f9f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.626228   71528 crypto.go:164] Writing key to /Users/mikhil/.minikube/ca.key ...
I0610 17:44:00.626237   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/ca.key: {Name:mkeb8aa3b9c980f0cf4b395d1434e94fcba6e89f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.626414   71528 certs.go:240] generating "proxyClientCA" ca cert: /Users/mikhil/.minikube/proxy-client-ca.key
I0610 17:44:00.726889   71528 crypto.go:156] Writing cert to /Users/mikhil/.minikube/proxy-client-ca.crt ...
I0610 17:44:00.726899   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/proxy-client-ca.crt: {Name:mk85d159ced70d81815e0ca135c6ba22fd425589 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.727180   71528 crypto.go:164] Writing key to /Users/mikhil/.minikube/proxy-client-ca.key ...
I0610 17:44:00.727182   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/proxy-client-ca.key: {Name:mk090e9a5f919c62a35da4cc3bc9985bbbf2f22e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.727335   71528 certs.go:256] generating profile certs ...
I0610 17:44:00.727375   71528 certs.go:363] generating signed profile cert for "minikube-user": /Users/mikhil/.minikube/profiles/minikube/client.key
I0610 17:44:00.727396   71528 crypto.go:68] Generating cert /Users/mikhil/.minikube/profiles/minikube/client.crt with IP's: []
I0610 17:44:00.923470   71528 crypto.go:156] Writing cert to /Users/mikhil/.minikube/profiles/minikube/client.crt ...
I0610 17:44:00.923485   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/client.crt: {Name:mked10b5230b9a346c89f998ef3f79c754166abb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.951315   71528 crypto.go:164] Writing key to /Users/mikhil/.minikube/profiles/minikube/client.key ...
I0610 17:44:00.951329   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/client.key: {Name:mk1e4cf6e89da126a363461be1e6214584432e33 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:00.967660   71528 certs.go:363] generating signed profile cert for "minikube": /Users/mikhil/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0610 17:44:00.967690   71528 crypto.go:68] Generating cert /Users/mikhil/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0610 17:44:01.030588   71528 crypto.go:156] Writing cert to /Users/mikhil/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0610 17:44:01.030599   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkdc1aa7c6fcf9d441c55e6fd5c802120f4b430f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:01.030880   71528 crypto.go:164] Writing key to /Users/mikhil/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0610 17:44:01.030883   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk882ec8bd11ac1d111e1a9dac7544b4fc66eca2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:01.031017   71528 certs.go:381] copying /Users/mikhil/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/mikhil/.minikube/profiles/minikube/apiserver.crt
I0610 17:44:01.031122   71528 certs.go:385] copying /Users/mikhil/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/mikhil/.minikube/profiles/minikube/apiserver.key
I0610 17:44:01.031220   71528 certs.go:363] generating signed profile cert for "aggregator": /Users/mikhil/.minikube/profiles/minikube/proxy-client.key
I0610 17:44:01.031231   71528 crypto.go:68] Generating cert /Users/mikhil/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0610 17:44:01.258551   71528 crypto.go:156] Writing cert to /Users/mikhil/.minikube/profiles/minikube/proxy-client.crt ...
I0610 17:44:01.258569   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/proxy-client.crt: {Name:mk002469f73370488accbc054ccda3da61c5c1a9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:01.258853   71528 crypto.go:164] Writing key to /Users/mikhil/.minikube/profiles/minikube/proxy-client.key ...
I0610 17:44:01.258856   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.minikube/profiles/minikube/proxy-client.key: {Name:mkfb1c0957ab48066807fafc52f3eb8a8ceba982 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:01.259144   71528 certs.go:484] found cert: /Users/mikhil/.minikube/certs/ca-key.pem (1679 bytes)
I0610 17:44:01.259347   71528 certs.go:484] found cert: /Users/mikhil/.minikube/certs/ca.pem (1078 bytes)
I0610 17:44:01.259395   71528 certs.go:484] found cert: /Users/mikhil/.minikube/certs/cert.pem (1123 bytes)
I0610 17:44:01.259525   71528 certs.go:484] found cert: /Users/mikhil/.minikube/certs/key.pem (1679 bytes)
I0610 17:44:01.265667   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0610 17:44:01.284573   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0610 17:44:01.297236   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0610 17:44:01.311167   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0610 17:44:01.324465   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0610 17:44:01.344639   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0610 17:44:01.361091   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0610 17:44:01.374890   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0610 17:44:01.385259   71528 ssh_runner.go:362] scp /Users/mikhil/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0610 17:44:01.395354   71528 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0610 17:44:01.402887   71528 ssh_runner.go:195] Run: openssl version
I0610 17:44:01.406563   71528 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0610 17:44:01.411244   71528 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0610 17:44:01.412909   71528 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 10 09:44 /usr/share/ca-certificates/minikubeCA.pem
I0610 17:44:01.412934   71528 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0610 17:44:01.416648   71528 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0610 17:44:01.421012   71528 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0610 17:44:01.422889   71528 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0610 17:44:01.422936   71528 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0610 17:44:01.422994   71528 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0610 17:44:01.432054   71528 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0610 17:44:01.436363   71528 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0610 17:44:01.440498   71528 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0610 17:44:01.440554   71528 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0610 17:44:01.444428   71528 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0610 17:44:01.444447   71528 kubeadm.go:156] found existing configuration files:

I0610 17:44:01.444496   71528 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0610 17:44:01.448373   71528 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0610 17:44:01.448434   71528 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0610 17:44:01.452311   71528 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0610 17:44:01.456173   71528 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0610 17:44:01.456222   71528 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0610 17:44:01.459945   71528 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0610 17:44:01.463917   71528 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0610 17:44:01.463969   71528 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0610 17:44:01.467999   71528 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0610 17:44:01.471715   71528 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0610 17:44:01.471763   71528 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0610 17:44:01.475423   71528 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0610 17:44:01.514345   71528 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0610 17:44:01.514451   71528 kubeadm.go:309] [preflight] Running pre-flight checks
I0610 17:44:01.579506   71528 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0610 17:44:01.579626   71528 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0610 17:44:01.579786   71528 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0610 17:44:01.738033   71528 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0610 17:44:01.750660   71528 out.go:204]     â–ª Generating certificates and keys ...
I0610 17:44:01.750731   71528 kubeadm.go:309] [certs] Using existing ca certificate authority
I0610 17:44:01.750769   71528 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0610 17:44:02.080023   71528 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0610 17:44:02.211699   71528 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0610 17:44:02.296645   71528 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0610 17:44:02.475521   71528 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0610 17:44:02.532979   71528 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0610 17:44:02.533140   71528 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0610 17:44:02.753761   71528 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0610 17:44:02.754164   71528 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0610 17:44:02.976880   71528 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0610 17:44:03.067704   71528 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0610 17:44:03.120842   71528 kubeadm.go:309] [certs] Generating "sa" key and public key
I0610 17:44:03.120933   71528 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0610 17:44:03.202735   71528 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0610 17:44:03.285361   71528 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0610 17:44:03.356640   71528 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0610 17:44:03.411921   71528 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0610 17:44:03.503827   71528 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0610 17:44:03.504736   71528 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0610 17:44:03.505697   71528 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0610 17:44:03.513605   71528 out.go:204]     â–ª Booting up control plane ...
I0610 17:44:03.513753   71528 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0610 17:44:03.513850   71528 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0610 17:44:03.513922   71528 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0610 17:44:03.514043   71528 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0610 17:44:03.514175   71528 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0610 17:44:03.514273   71528 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0610 17:44:03.569840   71528 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0610 17:44:03.569935   71528 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0610 17:44:04.571742   71528 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.001659667s
I0610 17:44:04.571832   71528 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0610 17:44:08.074689   71528 kubeadm.go:309] [api-check] The API server is healthy after 3.502379627s
I0610 17:44:08.086567   71528 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0610 17:44:08.091147   71528 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0610 17:44:08.101190   71528 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0610 17:44:08.101441   71528 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0610 17:44:08.108703   71528 kubeadm.go:309] [bootstrap-token] Using token: 2pupp7.30x8tlhppd27aq2q
I0610 17:44:08.119003   71528 out.go:204]     â–ª Configuring RBAC rules ...
I0610 17:44:08.119542   71528 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0610 17:44:08.119653   71528 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0610 17:44:08.128435   71528 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0610 17:44:08.138852   71528 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0610 17:44:08.138986   71528 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0610 17:44:08.139143   71528 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0610 17:44:08.482719   71528 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0610 17:44:08.901189   71528 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0610 17:44:09.480418   71528 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0610 17:44:09.481022   71528 kubeadm.go:309] 
I0610 17:44:09.481061   71528 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0610 17:44:09.481063   71528 kubeadm.go:309] 
I0610 17:44:09.481128   71528 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0610 17:44:09.481134   71528 kubeadm.go:309] 
I0610 17:44:09.481149   71528 kubeadm.go:309]   mkdir -p $HOME/.kube
I0610 17:44:09.481208   71528 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0610 17:44:09.481242   71528 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0610 17:44:09.481255   71528 kubeadm.go:309] 
I0610 17:44:09.481287   71528 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0610 17:44:09.481290   71528 kubeadm.go:309] 
I0610 17:44:09.481314   71528 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0610 17:44:09.481316   71528 kubeadm.go:309] 
I0610 17:44:09.481343   71528 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0610 17:44:09.481394   71528 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0610 17:44:09.481433   71528 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0610 17:44:09.481435   71528 kubeadm.go:309] 
I0610 17:44:09.481522   71528 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0610 17:44:09.481581   71528 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0610 17:44:09.481583   71528 kubeadm.go:309] 
I0610 17:44:09.481635   71528 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token 2pupp7.30x8tlhppd27aq2q \
I0610 17:44:09.481707   71528 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:293ff6fda021a149b2314043196665b04c9f3ae2c327c1a80507fa0f1ed5b8ec \
I0610 17:44:09.481720   71528 kubeadm.go:309] 	--control-plane 
I0610 17:44:09.481723   71528 kubeadm.go:309] 
I0610 17:44:09.481773   71528 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0610 17:44:09.481775   71528 kubeadm.go:309] 
I0610 17:44:09.481823   71528 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token 2pupp7.30x8tlhppd27aq2q \
I0610 17:44:09.481888   71528 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:293ff6fda021a149b2314043196665b04c9f3ae2c327c1a80507fa0f1ed5b8ec 
I0610 17:44:09.484308   71528 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0610 17:44:09.484361   71528 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0610 17:44:09.484392   71528 cni.go:84] Creating CNI manager for ""
I0610 17:44:09.484413   71528 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0610 17:44:09.486181   71528 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0610 17:44:09.498756   71528 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0610 17:44:09.516885   71528 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0610 17:44:09.527426   71528 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0610 17:44:09.527629   71528 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0610 17:44:09.527616   71528 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_06_10T17_44_09_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0610 17:44:09.613242   71528 kubeadm.go:1107] duration metric: took 85.733834ms to wait for elevateKubeSystemPrivileges
I0610 17:44:09.613267   71528 ops.go:34] apiserver oom_adj: -16
W0610 17:44:09.626041   71528 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0610 17:44:09.626077   71528 kubeadm.go:393] duration metric: took 8.203256042s to StartCluster
I0610 17:44:09.626125   71528 settings.go:142] acquiring lock: {Name:mk3bc76c1eed2b943a0cbb54e8e0035b83edbfd4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:09.626592   71528 settings.go:150] Updating kubeconfig:  /Users/mikhil/.kube/config
I0610 17:44:09.629022   71528 lock.go:35] WriteFile acquiring /Users/mikhil/.kube/config: {Name:mkb33459fd035a3acad7a4a80cad5190bd512740 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0610 17:44:09.629790   71528 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0610 17:44:09.630070   71528 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0610 17:44:09.641159   71528 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0610 17:44:09.633517   71528 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0610 17:44:09.632836   71528 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0610 17:44:09.648956   71528 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0610 17:44:09.649342   71528 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0610 17:44:09.649489   71528 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0610 17:44:09.649544   71528 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0610 17:44:09.649674   71528 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0610 17:44:09.652811   71528 host.go:66] Checking if "minikube" exists ...
I0610 17:44:09.656586   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:44:09.658015   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:44:09.736545   71528 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0610 17:44:09.766130   71528 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0610 17:44:09.770593   71528 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0610 17:44:09.770599   71528 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0610 17:44:09.770672   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:44:09.779238   71528 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0610 17:44:09.779273   71528 host.go:66] Checking if "minikube" exists ...
I0610 17:44:09.779625   71528 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0610 17:44:09.833921   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:44:09.847501   71528 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0610 17:44:09.847509   71528 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0610 17:44:09.847637   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0610 17:44:09.889286   71528 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56204 SSHKeyPath:/Users/mikhil/.minikube/machines/minikube/id_rsa Username:docker}
I0610 17:44:09.909031   71528 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0610 17:44:10.111867   71528 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0610 17:44:10.112727   71528 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0610 17:44:10.200160   71528 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0610 17:44:10.200345   71528 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0610 17:44:10.259872   71528 api_server.go:52] waiting for apiserver process to appear ...
I0610 17:44:10.259934   71528 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0610 17:44:10.297608   71528 api_server.go:72] duration metric: took 667.511833ms to wait for apiserver process to appear ...
I0610 17:44:10.297622   71528 api_server.go:88] waiting for apiserver healthz status ...
I0610 17:44:10.297839   71528 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56208/healthz ...
I0610 17:44:10.308666   71528 out.go:177] ðŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
I0610 17:44:10.316484   71528 addons.go:505] duration metric: took 686.083208ms for enable addons: enabled=[default-storageclass storage-provisioner]
I0610 17:44:10.313859   71528 api_server.go:279] https://127.0.0.1:56208/healthz returned 200:
ok
I0610 17:44:10.318329   71528 api_server.go:141] control plane version: v1.30.0
I0610 17:44:10.318339   71528 api_server.go:131] duration metric: took 20.715208ms to wait for apiserver health ...
I0610 17:44:10.318344   71528 system_pods.go:43] waiting for kube-system pods to appear ...
I0610 17:44:10.324438   71528 system_pods.go:59] 5 kube-system pods found
I0610 17:44:10.324449   71528 system_pods.go:61] "etcd-minikube" [21fffb3d-e0e7-4e27-9784-c64afd3dc1c5] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0610 17:44:10.324457   71528 system_pods.go:61] "kube-apiserver-minikube" [97c9bf09-2978-4809-9cbd-d8bc0c5defd6] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0610 17:44:10.324461   71528 system_pods.go:61] "kube-controller-manager-minikube" [a6d66ff4-4736-416a-b41d-8b7b8b046c71] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0610 17:44:10.324463   71528 system_pods.go:61] "kube-scheduler-minikube" [996277ce-3327-4efd-b485-eb390a819ac6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0610 17:44:10.324464   71528 system_pods.go:61] "storage-provisioner" [7df86d40-b51a-4626-a5a8-741a1a4eb443] Pending
I0610 17:44:10.324468   71528 system_pods.go:74] duration metric: took 6.120917ms to wait for pod list to return data ...
I0610 17:44:10.324473   71528 kubeadm.go:576] duration metric: took 694.381083ms to wait for: map[apiserver:true system_pods:true]
I0610 17:44:10.324479   71528 node_conditions.go:102] verifying NodePressure condition ...
I0610 17:44:10.327027   71528 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0610 17:44:10.327034   71528 node_conditions.go:123] node cpu capacity is 8
I0610 17:44:10.327048   71528 node_conditions.go:105] duration metric: took 2.567959ms to run NodePressure ...
I0610 17:44:10.327054   71528 start.go:240] waiting for startup goroutines ...
I0610 17:44:10.712288   71528 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0610 17:44:10.712324   71528 start.go:245] waiting for cluster config update ...
I0610 17:44:10.712345   71528 start.go:254] writing updated cluster config ...
I0610 17:44:10.716403   71528 ssh_runner.go:195] Run: rm -f paused
I0610 17:44:11.543666   71528 start.go:600] kubectl: 1.30.1, cluster: 1.30.0 (minor skew: 0)
I0610 17:44:11.548697   71528 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 10 09:43:59 minikube dockerd[1071]: time="2024-06-10T09:43:59.634936009Z" level=info msg="Loading containers: done."
Jun 10 09:43:59 minikube dockerd[1071]: time="2024-06-10T09:43:59.639980384Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jun 10 09:43:59 minikube dockerd[1071]: time="2024-06-10T09:43:59.640029259Z" level=info msg="Daemon has completed initialization"
Jun 10 09:43:59 minikube dockerd[1071]: time="2024-06-10T09:43:59.657272884Z" level=info msg="API listen on /var/run/docker.sock"
Jun 10 09:43:59 minikube dockerd[1071]: time="2024-06-10T09:43:59.657334175Z" level=info msg="API listen on [::]:2376"
Jun 10 09:43:59 minikube systemd[1]: Started Docker Application Container Engine.
Jun 10 09:43:59 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Start docker client with request timeout 0s"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Loaded network plugin cni"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 10 09:43:59 minikube cri-dockerd[1296]: time="2024-06-10T09:43:59Z" level=info msg="Start cri-dockerd grpc backend"
Jun 10 09:43:59 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 10 09:44:04 minikube cri-dockerd[1296]: time="2024-06-10T09:44:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1afa3349127298e0c7ec4dfbe7722cb6fca3db4340c1c29b8b57e2ac3bdc97a8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:04 minikube cri-dockerd[1296]: time="2024-06-10T09:44:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6e1a3eb23f7d7b6a3aaddd61d75db7c89185ded9b2c520055abf0c7eb8a51ea9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:04 minikube cri-dockerd[1296]: time="2024-06-10T09:44:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0a1749650fe45a873f3e9ddd1e17a64b33ef45839b6130c8f3899fb2c4cadf0e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:04 minikube cri-dockerd[1296]: time="2024-06-10T09:44:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2b7ffed0bc6da3747508dae0d4dbf40534285a693d30dfb84054de4218d22b56/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:23 minikube cri-dockerd[1296]: time="2024-06-10T09:44:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c14b732f13fa91d8106573f6d78d71922458b7b6806b0a96b842adb2044ae5df/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:23 minikube cri-dockerd[1296]: time="2024-06-10T09:44:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6a4031bd8ac65c9952574b82511a7367a19a368f8e8b6f2e89f66e5a345142e1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:24 minikube cri-dockerd[1296]: time="2024-06-10T09:44:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b3255afc807df0fae2a0e267ee8291a098201dc399d076b38cce9b7a99cc08d3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 10 09:44:29 minikube cri-dockerd[1296]: time="2024-06-10T09:44:29Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 10 09:46:40 minikube cri-dockerd[1296]: time="2024-06-10T09:46:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/889d4a57dd6fd9110b4417a80a7e2e59338777b2efcdf3051d0da17cdf5db10e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 10 09:46:40 minikube cri-dockerd[1296]: time="2024-06-10T09:46:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54f5269169364e212d1388c3044bef202b59b5d9ef6d5be20e5e926f7735600d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 10 09:46:40 minikube cri-dockerd[1296]: time="2024-06-10T09:46:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e940783a431a35c2748a14ad790c68d1688ae2d345082f1c97b27865bbb7c9de/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 10 09:46:43 minikube dockerd[1071]: time="2024-06-10T09:46:43.802592293Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=4bd82deda8a9c8aa traceID=67924c0217a254b9919386b168356c2d
Jun 10 09:46:43 minikube dockerd[1071]: time="2024-06-10T09:46:43.802906418Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:46:47 minikube dockerd[1071]: time="2024-06-10T09:46:47.031303294Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=4b86ac61ab12bdde traceID=d3c9207c4f8289a1221eddf33fc46309
Jun 10 09:46:47 minikube dockerd[1071]: time="2024-06-10T09:46:47.031678253Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:46:50 minikube dockerd[1071]: time="2024-06-10T09:46:50.148133171Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=4ae928e13715655a traceID=1905381025bdce6740e3b3a4311756f2
Jun 10 09:46:50 minikube dockerd[1071]: time="2024-06-10T09:46:50.150006754Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:01 minikube dockerd[1071]: time="2024-06-10T09:47:01.737497968Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=45479ebb03ba68b6 traceID=bf317e2cfc8a83277f8ef75ed85eabae
Jun 10 09:47:01 minikube dockerd[1071]: time="2024-06-10T09:47:01.737753468Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:04 minikube dockerd[1071]: time="2024-06-10T09:47:04.789177886Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=5df96a91eed54034 traceID=56338d860947b6a1a2306c9833e16e02
Jun 10 09:47:04 minikube dockerd[1071]: time="2024-06-10T09:47:04.789319386Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:07 minikube dockerd[1071]: time="2024-06-10T09:47:07.930046596Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=f102db89949f4e83 traceID=b39b0590f6106ce2a3bcb19748bc8587
Jun 10 09:47:07 minikube dockerd[1071]: time="2024-06-10T09:47:07.930241096Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:31 minikube dockerd[1071]: time="2024-06-10T09:47:31.833016843Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b139f26019e54a9e traceID=df24fbec8ccb2a82933dc52550adb873
Jun 10 09:47:31 minikube dockerd[1071]: time="2024-06-10T09:47:31.833259385Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:34 minikube dockerd[1071]: time="2024-06-10T09:47:34.871181011Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3434dbfe946d1e5f traceID=b9bcc5626000c761e2eda0808e4db103
Jun 10 09:47:34 minikube dockerd[1071]: time="2024-06-10T09:47:34.871305845Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:47:38 minikube dockerd[1071]: time="2024-06-10T09:47:38.799429180Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=6bf6a5145d624abf traceID=364c662ae29e25349e44205cf0acef7b
Jun 10 09:47:38 minikube dockerd[1071]: time="2024-06-10T09:47:38.799634888Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:06 minikube cri-dockerd[1296]: time="2024-06-10T09:48:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6de2f7e7902633f955839151b8d87d4c2960bbbf95c2805a274b5f722c563564/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 10 09:48:09 minikube dockerd[1071]: time="2024-06-10T09:48:09.818659472Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=d5748e5df577b863 traceID=8e41c77e6011525f347ba8ee958afe2c
Jun 10 09:48:09 minikube dockerd[1071]: time="2024-06-10T09:48:09.818866639Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:23 minikube dockerd[1071]: time="2024-06-10T09:48:23.022581297Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=701a73026f743d79 traceID=f4dc2a22337e93e3be9d89ee5b142c27
Jun 10 09:48:23 minikube dockerd[1071]: time="2024-06-10T09:48:23.022778214Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:26 minikube dockerd[1071]: time="2024-06-10T09:48:26.393219507Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=32006e24c58c50d9 traceID=c24e135590102b2ce9708f3637c85ff9
Jun 10 09:48:26 minikube dockerd[1071]: time="2024-06-10T09:48:26.393418674Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:29 minikube dockerd[1071]: time="2024-06-10T09:48:29.399265467Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=412239f6afeb8e07 traceID=102a136e6d6ee9fdc1b1fc2ccf9638ae
Jun 10 09:48:29 minikube dockerd[1071]: time="2024-06-10T09:48:29.399629175Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:32 minikube dockerd[1071]: time="2024-06-10T09:48:32.344868510Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e8bd0d07fecbcedf traceID=77dff68a74b985aefc43155f92c008df
Jun 10 09:48:32 minikube dockerd[1071]: time="2024-06-10T09:48:32.344946052Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 10 09:48:56 minikube dockerd[1071]: time="2024-06-10T09:48:56.649984591Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=d0c1e46c7b6aa890 traceID=571acbd055a0b4b2b96bc894c0d6c1ea
Jun 10 09:48:56 minikube dockerd[1071]: time="2024-06-10T09:48:56.650209466Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
b416e196bd58e       ba04bb24b9575       5 minutes ago       Running             storage-provisioner       0                   b3255afc807df       storage-provisioner
909758ddcd6d4       cb7eac0b42cc1       5 minutes ago       Running             kube-proxy                0                   6a4031bd8ac65       kube-proxy-wwgzx
bb75815693eb2       2437cf7621777       5 minutes ago       Running             coredns                   0                   c14b732f13fa9       coredns-7db6d8ff4d-q9sqx
a47978f971407       014faa467e297       5 minutes ago       Running             etcd                      0                   2b7ffed0bc6da       etcd-minikube
b9b2e34ad5a04       181f57fd3cdb7       5 minutes ago       Running             kube-apiserver            0                   1afa334912729       kube-apiserver-minikube
a7a428c50fcce       68feac521c0f1       5 minutes ago       Running             kube-controller-manager   0                   0a1749650fe45       kube-controller-manager-minikube
e7ab994a38c54       547adae34140b       5 minutes ago       Running             kube-scheduler            0                   6e1a3eb23f7d7       kube-scheduler-minikube


==> coredns [bb75815693eb] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:41467 - 26370 "HINFO IN 3553629741348457777.4717702812455154029. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.031024334s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2055330727]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 09:44:23.531) (total time: 30007ms):
Trace[2055330727]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (09:44:53.534)
Trace[2055330727]: [30.007464972s] [30.007464972s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1287767478]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 09:44:23.531) (total time: 30007ms):
Trace[1287767478]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (09:44:53.537)
Trace[1287767478]: [30.007825472s] [30.007825472s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1504140675]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 09:44:23.531) (total time: 30008ms):
Trace[1504140675]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30006ms (09:44:53.537)
Trace[1504140675]: [30.008141389s] [30.008141389s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_10T17_44_09_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Jun 2024 09:44:06 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 10 Jun 2024 09:49:45 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 10 Jun 2024 09:49:35 +0000   Mon, 10 Jun 2024 09:44:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 10 Jun 2024 09:49:35 +0000   Mon, 10 Jun 2024 09:44:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 10 Jun 2024 09:49:35 +0000   Mon, 10 Jun 2024 09:44:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 10 Jun 2024 09:49:35 +0000   Mon, 10 Jun 2024 09:44:06 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029248Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8029248Ki
  pods:               110
System Info:
  Machine ID:                 b01da97fc6c3413fa0e87ef42fa25c7d
  System UUID:                b01da97fc6c3413fa0e87ef42fa25c7d
  Boot ID:                    2659a54e-08bf-454f-9b92-256e4658e495
  Kernel Version:             6.6.26-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     flask-app-55fcc87c55-crc8j          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m15s
  default                     flask-app-55fcc87c55-gn7rr          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m15s
  default                     flask-app-55fcc87c55-jck7x          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m15s
  default                     flask-app-77df5f7bff-qcxcr          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         108s
  kube-system                 coredns-7db6d8ff4d-q9sqx            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     5m32s
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         5m46s
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m46s
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m46s
  kube-system                 kube-proxy-wwgzx                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m32s
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m46s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         5m44s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 5m30s                  kube-proxy       
  Normal  Starting                 5m50s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  5m50s (x8 over 5m50s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m50s (x8 over 5m50s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m50s (x7 over 5m50s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m50s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 5m46s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  5m46s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  5m46s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m46s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m46s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           5m33s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun10 09:41] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.161150] netlink: 'init': attribute type 4 has an invalid length.
[  +0.014586] fakeowner: loading out-of-tree module taints kernel.
[  +0.037438] setting FUSE negative_dentry_timeout to 3600 seconds
[  +0.000005] setting FUSE entry_timeout to 3600 seconds
[  +0.000000] setting FUSE attr_timeout to 3600 seconds
[  +0.000001] ignoring STATX_ATIME
[  +0.000000] returning ENOSYS from FUSE_FLUSH
[  +0.000001] overriding FOPEN_KEEP_CACHE
[  +1.274405] netlink: 'init': attribute type 22 has an invalid length.
[Jun10 09:43] systemd[917]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set


==> etcd [a47978f97140] <==
{"level":"warn","ts":"2024-06-10T09:44:04.939481Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-10T09:44:04.939806Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-06-10T09:44:04.940008Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-10T09:44:04.940016Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-10T09:44:04.940041Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-10T09:44:04.941219Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-10T09:44:04.941333Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-10T09:44:04.94571Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"4.142833ms"}
{"level":"info","ts":"2024-06-10T09:44:05.038766Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-06-10T09:44:05.038844Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-06-10T09:44:05.039673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-06-10T09:44:05.039679Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-06-10T09:44:05.039682Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-06-10T09:44:05.039705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-06-10T09:44:05.059179Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-10T09:44:05.065898Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-06-10T09:44:05.066542Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-10T09:44:05.10207Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-06-10T09:44:05.104295Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-06-10T09:44:05.105107Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-10T09:44:05.105643Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-10T09:44:05.105716Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-10T09:44:05.111319Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-10T09:44:05.111573Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-10T09:44:05.113948Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-10T09:44:05.114253Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-10T09:44:05.114443Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-10T09:44:05.116787Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-10T09:44:05.116818Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-10T09:44:05.641667Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-06-10T09:44:05.641704Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-06-10T09:44:05.641713Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-06-10T09:44:05.641728Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-06-10T09:44:05.641733Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-06-10T09:44:05.641738Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-06-10T09:44:05.641743Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-06-10T09:44:05.650441Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-10T09:44:05.659154Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-10T09:44:05.659204Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-10T09:44:05.659365Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-10T09:44:05.659883Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-10T09:44:05.659998Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-10T09:44:05.659879Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-10T09:44:05.660108Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-10T09:44:05.660126Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-10T09:44:05.661224Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-10T09:44:05.661324Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}


==> kernel <==
 09:49:54 up 8 min,  0 users,  load average: 3.59, 2.85, 1.42
Linux minikube 6.6.26-linuxkit #1 SMP Sat Apr 27 04:13:19 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [b9b2e34ad5a0] <==
I0610 09:44:06.327683       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0610 09:44:06.327890       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0610 09:44:06.327950       1 controller.go:139] Starting OpenAPI controller
I0610 09:44:06.327979       1 controller.go:87] Starting OpenAPI V3 controller
I0610 09:44:06.328003       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0610 09:44:06.328026       1 naming_controller.go:291] Starting NamingConditionController
I0610 09:44:06.328041       1 establishing_controller.go:76] Starting EstablishingController
I0610 09:44:06.328068       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0610 09:44:06.328075       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0610 09:44:06.328080       1 crd_finalizer.go:266] Starting CRDFinalizer
I0610 09:44:06.328094       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0610 09:44:06.328100       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0610 09:44:06.328103       1 available_controller.go:423] Starting AvailableConditionController
I0610 09:44:06.328114       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0610 09:44:06.328123       1 aggregator.go:163] waiting for initial CRD sync...
I0610 09:44:06.328128       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0610 09:44:06.328130       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0610 09:44:06.328152       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0610 09:44:06.328156       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0610 09:44:06.328163       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0610 09:44:06.328898       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0610 09:44:06.329077       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0610 09:44:06.329248       1 controller.go:78] Starting OpenAPI AggregationController
I0610 09:44:06.329982       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0610 09:44:06.330016       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0610 09:44:06.328096       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0610 09:44:06.347291       1 shared_informer.go:320] Caches are synced for node_authorizer
I0610 09:44:06.351634       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0610 09:44:06.351657       1 policy_source.go:224] refreshing policies
E0610 09:44:06.394944       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0610 09:44:06.427946       1 shared_informer.go:320] Caches are synced for configmaps
I0610 09:44:06.428143       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0610 09:44:06.428152       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0610 09:44:06.428143       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0610 09:44:06.428164       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0610 09:44:06.428174       1 aggregator.go:165] initial CRD sync complete...
I0610 09:44:06.428676       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0610 09:44:06.428890       1 autoregister_controller.go:141] Starting autoregister controller
I0610 09:44:06.428908       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0610 09:44:06.428911       1 cache.go:39] Caches are synced for autoregister controller
I0610 09:44:06.429586       1 controller.go:615] quota admission added evaluator for: namespaces
I0610 09:44:06.430685       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0610 09:44:06.430699       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0610 09:44:06.599788       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0610 09:44:07.331694       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0610 09:44:07.333775       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0610 09:44:07.333789       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0610 09:44:07.548173       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0610 09:44:07.568739       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0610 09:44:07.656002       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0610 09:44:07.669414       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0610 09:44:07.670349       1 controller.go:615] quota admission added evaluator for: endpoints
I0610 09:44:07.673503       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0610 09:44:08.360206       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0610 09:44:08.893579       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0610 09:44:08.899656       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0610 09:44:08.904857       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0610 09:44:22.360343       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0610 09:44:22.516428       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0610 09:46:50.506915       1 alloc.go:330] "allocated clusterIPs" service="default/flask-app-service" clusterIPs={"IPv4":"10.99.26.35"}


==> kube-controller-manager [a7a428c50fcc] <==
I0610 09:44:21.621074       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0610 09:44:21.621076       1 shared_informer.go:320] Caches are synced for cidrallocator
I0610 09:44:21.624892       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0610 09:44:21.650131       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0610 09:44:21.667003       1 shared_informer.go:320] Caches are synced for ReplicationController
I0610 09:44:21.702077       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0610 09:44:21.711106       1 shared_informer.go:320] Caches are synced for disruption
I0610 09:44:21.752137       1 shared_informer.go:320] Caches are synced for crt configmap
I0610 09:44:21.784048       1 shared_informer.go:320] Caches are synced for resource quota
I0610 09:44:21.817073       1 shared_informer.go:320] Caches are synced for resource quota
I0610 09:44:22.230136       1 shared_informer.go:320] Caches are synced for garbage collector
I0610 09:44:22.236311       1 shared_informer.go:320] Caches are synced for garbage collector
I0610 09:44:22.236331       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0610 09:44:22.721213       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="202.664417ms"
I0610 09:44:22.725487       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.202667ms"
I0610 09:44:22.725765       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="99.583Âµs"
I0610 09:44:22.732675       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="23.459Âµs"
I0610 09:44:24.007869       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="109.709Âµs"
I0610 09:45:03.076242       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="12.360292ms"
I0610 09:45:03.077024       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="302.625Âµs"
I0610 09:46:39.970485       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="11.165083ms"
I0610 09:46:39.974337       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="3.807917ms"
I0610 09:46:39.974389       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="28.917Âµs"
I0610 09:46:39.976037       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="23.625Âµs"
I0610 09:46:39.981022       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="17.042Âµs"
I0610 09:46:39.984394       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="25.583Âµs"
I0610 09:46:44.861599       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="87.625Âµs"
I0610 09:46:47.915107       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="254.708Âµs"
I0610 09:46:50.964345       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="85.333Âµs"
I0610 09:46:58.689216       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="90.375Âµs"
I0610 09:46:59.690717       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="148.625Âµs"
I0610 09:47:01.684527       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="94.917Âµs"
I0610 09:47:15.684459       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="153.916Âµs"
I0610 09:47:16.684565       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="77.375Âµs"
I0610 09:47:22.687512       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="106.167Âµs"
I0610 09:47:28.686600       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="136.833Âµs"
I0610 09:47:29.688050       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="97.625Âµs"
I0610 09:47:35.685115       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="115.25Âµs"
I0610 09:47:43.674611       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="76.583Âµs"
I0610 09:47:47.681943       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="79.041Âµs"
I0610 09:47:53.671744       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="54.417Âµs"
I0610 09:47:54.680663       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="114.375Âµs"
I0610 09:47:59.674613       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="97.125Âµs"
I0610 09:48:06.141850       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="6.503375ms"
I0610 09:48:06.146314       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="4.432458ms"
I0610 09:48:06.146367       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="29.583Âµs"
I0610 09:48:06.147347       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="12.792Âµs"
I0610 09:48:07.685514       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="141.5Âµs"
I0610 09:48:10.119673       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="70.459Âµs"
I0610 09:48:21.674411       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="316.791Âµs"
I0610 09:48:36.719703       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="742.959Âµs"
I0610 09:48:38.674333       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="183.208Âµs"
I0610 09:48:40.679118       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="45.958Âµs"
I0610 09:48:45.674974       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="70.125Âµs"
I0610 09:48:48.680184       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="45.375Âµs"
I0610 09:48:53.687035       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="42.666Âµs"
I0610 09:48:53.694598       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="36.416Âµs"
I0610 09:49:00.674642       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-55fcc87c55" duration="35.75Âµs"
I0610 09:49:08.681970       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="277.875Âµs"
I0610 09:49:19.685994       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-app-77df5f7bff" duration="154.084Âµs"


==> kube-proxy [909758ddcd6d] <==
I0610 09:44:23.536947       1 server_linux.go:69] "Using iptables proxy"
I0610 09:44:23.544486       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0610 09:44:23.555502       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0610 09:44:23.555528       1 server_linux.go:165] "Using iptables Proxier"
I0610 09:44:23.556500       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0610 09:44:23.556511       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0610 09:44:23.556550       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0610 09:44:23.556718       1 server.go:872] "Version info" version="v1.30.0"
I0610 09:44:23.556738       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0610 09:44:23.557734       1 config.go:192] "Starting service config controller"
I0610 09:44:23.557757       1 shared_informer.go:313] Waiting for caches to sync for service config
I0610 09:44:23.557781       1 config.go:101] "Starting endpoint slice config controller"
I0610 09:44:23.557783       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0610 09:44:23.557934       1 config.go:319] "Starting node config controller"
I0610 09:44:23.557956       1 shared_informer.go:313] Waiting for caches to sync for node config
I0610 09:44:23.658758       1 shared_informer.go:320] Caches are synced for node config
I0610 09:44:23.658790       1 shared_informer.go:320] Caches are synced for service config
I0610 09:44:23.658760       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [e7ab994a38c5] <==
I0610 09:44:05.333779       1 serving.go:380] Generated self-signed cert in-memory
W0610 09:44:06.337977       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0610 09:44:06.338222       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0610 09:44:06.338237       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0610 09:44:06.338244       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0610 09:44:06.345886       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0610 09:44:06.345899       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0610 09:44:06.346841       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0610 09:44:06.346898       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0610 09:44:06.346932       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0610 09:44:06.346957       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0610 09:44:06.347708       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:06.347735       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0610 09:44:06.348784       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0610 09:44:06.348814       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0610 09:44:06.348861       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:06.348868       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0610 09:44:06.348928       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0610 09:44:06.348935       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0610 09:44:06.348954       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0610 09:44:06.348959       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0610 09:44:06.348984       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0610 09:44:06.348989       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0610 09:44:06.349056       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0610 09:44:06.349066       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0610 09:44:06.349094       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0610 09:44:06.349100       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0610 09:44:06.350704       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0610 09:44:06.350718       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0610 09:44:06.353970       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:06.353985       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0610 09:44:06.353992       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0610 09:44:06.353997       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:06.354009       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0610 09:44:06.354019       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0610 09:44:06.354033       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0610 09:44:06.354011       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0610 09:44:06.354030       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0610 09:44:06.354068       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0610 09:44:06.354092       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0610 09:44:06.354107       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0610 09:44:07.172983       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0610 09:44:07.173043       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0610 09:44:07.179110       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0610 09:44:07.179140       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0610 09:44:07.194437       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0610 09:44:07.194463       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0610 09:44:07.394290       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:07.394326       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0610 09:44:07.410132       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0610 09:44:07.410168       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0610 09:44:07.422198       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0610 09:44:07.422230       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0610 09:44:07.433043       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0610 09:44:07.433073       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0610 09:44:07.436945       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0610 09:44:07.436967       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I0610 09:44:07.648842       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 10 09:47:43 minikube kubelet[2148]: E0610 09:47:43.663522    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:47:47 minikube kubelet[2148]: E0610 09:47:47.670519    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:47:53 minikube kubelet[2148]: E0610 09:47:53.661950    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:47:54 minikube kubelet[2148]: E0610 09:47:54.670047    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:47:59 minikube kubelet[2148]: E0610 09:47:59.661879    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:48:06 minikube kubelet[2148]: I0610 09:48:06.142624    2148 topology_manager.go:215] "Topology Admit Handler" podUID="490fa563-aee0-43f8-8ec1-557d18eba913" podNamespace="default" podName="flask-app-77df5f7bff-qcxcr"
Jun 10 09:48:06 minikube kubelet[2148]: I0610 09:48:06.223026    2148 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lmv55\" (UniqueName: \"kubernetes.io/projected/490fa563-aee0-43f8-8ec1-557d18eba913-kube-api-access-lmv55\") pod \"flask-app-77df5f7bff-qcxcr\" (UID: \"490fa563-aee0-43f8-8ec1-557d18eba913\") " pod="default/flask-app-77df5f7bff-qcxcr"
Jun 10 09:48:07 minikube kubelet[2148]: E0610 09:48:07.668909    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:48:08 minikube kubelet[2148]: E0610 09:48:08.663367    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:48:09 minikube kubelet[2148]: E0610 09:48:09.828593    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:09 minikube kubelet[2148]: E0610 09:48:09.828765    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:09 minikube kubelet[2148]: E0610 09:48:09.829144    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:kuber_test:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmv55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-77df5f7bff-qcxcr_default(490fa563-aee0-43f8-8ec1-557d18eba913): ErrImagePull: Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:09 minikube kubelet[2148]: E0610 09:48:09.829280    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:48:10 minikube kubelet[2148]: E0610 09:48:10.105082    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"kuber_test:latest\\\"\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:48:10 minikube kubelet[2148]: E0610 09:48:10.668817    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:48:23 minikube kubelet[2148]: E0610 09:48:23.035067    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:23 minikube kubelet[2148]: E0610 09:48:23.035261    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:23 minikube kubelet[2148]: E0610 09:48:23.035949    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tk42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-55fcc87c55-jck7x_default(dfd68c6f-bc45-4076-88a5-74e9df29e6bb): ErrImagePull: Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:23 minikube kubelet[2148]: E0610 09:48:23.036039    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:48:26 minikube kubelet[2148]: E0610 09:48:26.403333    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:26 minikube kubelet[2148]: E0610 09:48:26.403557    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:26 minikube kubelet[2148]: E0610 09:48:26.404136    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:kuber_test:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmv55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-77df5f7bff-qcxcr_default(490fa563-aee0-43f8-8ec1-557d18eba913): ErrImagePull: Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:26 minikube kubelet[2148]: E0610 09:48:26.404232    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:48:29 minikube kubelet[2148]: E0610 09:48:29.409320    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:29 minikube kubelet[2148]: E0610 09:48:29.409424    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:29 minikube kubelet[2148]: E0610 09:48:29.409764    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6qw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-55fcc87c55-crc8j_default(a148140b-dba4-4a57-9606-57e57efc7115): ErrImagePull: Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:29 minikube kubelet[2148]: E0610 09:48:29.409810    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:48:32 minikube kubelet[2148]: E0610 09:48:32.349838    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:32 minikube kubelet[2148]: E0610 09:48:32.349982    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:48:32 minikube kubelet[2148]: E0610 09:48:32.350357    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8ksd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-55fcc87c55-gn7rr_default(2c300190-a6b2-43af-8fd0-d16536ee8907): ErrImagePull: Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:32 minikube kubelet[2148]: E0610 09:48:32.350463    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:48:36 minikube kubelet[2148]: E0610 09:48:36.689682    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:48:38 minikube kubelet[2148]: E0610 09:48:38.669207    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"kuber_test:latest\\\"\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:48:40 minikube kubelet[2148]: E0610 09:48:40.667931    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:48:45 minikube kubelet[2148]: E0610 09:48:45.661026    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:48:48 minikube kubelet[2148]: E0610 09:48:48.676201    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:48:53 minikube kubelet[2148]: E0610 09:48:53.665718    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:48:56 minikube kubelet[2148]: E0610 09:48:56.658204    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:56 minikube kubelet[2148]: E0610 09:48:56.658362    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kuber_test:latest"
Jun 10 09:48:56 minikube kubelet[2148]: E0610 09:48:56.658612    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:kuber_test:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmv55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-77df5f7bff-qcxcr_default(490fa563-aee0-43f8-8ec1-557d18eba913): ErrImagePull: Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:48:56 minikube kubelet[2148]: E0610 09:48:56.658696    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for kuber_test, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:48:59 minikube kubelet[2148]: E0610 09:48:59.661439    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:49:00 minikube kubelet[2148]: E0610 09:49:00.669122    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:49:07 minikube kubelet[2148]: E0610 09:49:07.670153    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:49:08 minikube kubelet[2148]: E0610 09:49:08.672425    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"kuber_test:latest\\\"\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:49:11 minikube kubelet[2148]: E0610 09:49:11.666101    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:49:12 minikube kubelet[2148]: E0610 09:49:12.669245    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:49:19 minikube kubelet[2148]: E0610 09:49:19.668068    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"kuber_test:latest\\\"\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:49:21 minikube kubelet[2148]: E0610 09:49:21.664730    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:49:24 minikube kubelet[2148]: E0610 09:49:24.667930    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:49:25 minikube kubelet[2148]: E0610 09:49:25.666943    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:49:32 minikube kubelet[2148]: E0610 09:49:32.668506    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:49:34 minikube kubelet[2148]: E0610 09:49:34.664307    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"kuber_test:latest\\\"\"" pod="default/flask-app-77df5f7bff-qcxcr" podUID="490fa563-aee0-43f8-8ec1-557d18eba913"
Jun 10 09:49:37 minikube kubelet[2148]: E0610 09:49:37.665826    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"
Jun 10 09:49:39 minikube kubelet[2148]: E0610 09:49:39.666272    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-gn7rr" podUID="2c300190-a6b2-43af-8fd0-d16536ee8907"
Jun 10 09:49:47 minikube kubelet[2148]: E0610 09:49:47.677782    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-app-55fcc87c55-crc8j" podUID="a148140b-dba4-4a57-9606-57e57efc7115"
Jun 10 09:49:54 minikube kubelet[2148]: E0610 09:49:54.038838    2148 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:49:54 minikube kubelet[2148]: E0610 09:49:54.038908    2148 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Jun 10 09:49:54 minikube kubelet[2148]: E0610 09:49:54.039168    2148 kuberuntime_manager.go:1256] container &Container{Name:flask-app,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tk42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flask-app-55fcc87c55-jck7x_default(dfd68c6f-bc45-4076-88a5-74e9df29e6bb): ErrImagePull: Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 10 09:49:54 minikube kubelet[2148]: E0610 09:49:54.039210    2148 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app\" with ErrImagePull: \"Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-app-55fcc87c55-jck7x" podUID="dfd68c6f-bc45-4076-88a5-74e9df29e6bb"


==> storage-provisioner [b416e196bd58] <==
I0610 09:44:24.222876       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0610 09:44:24.242660       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0610 09:44:24.243028       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0610 09:44:24.251520       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0610 09:44:24.251663       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f7b3d7f8-b3e2-4d84-ab59-acce5c5cbdce!
I0610 09:44:24.252320       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e201d3a8-940f-471d-b891-0d8020e5df2d", APIVersion:"v1", ResourceVersion:"410", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f7b3d7f8-b3e2-4d84-ab59-acce5c5cbdce became leader
I0610 09:44:24.353163       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f7b3d7f8-b3e2-4d84-ab59-acce5c5cbdce!

